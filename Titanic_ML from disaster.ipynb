{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data=np.genfromtxt('C:/Users/ACER M5/Downloads/titanic/train.csv',delimiter=',',usecols=[0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "raw_data=pd.DataFrame(pd.read_csv('train.csv'))\n",
    "unscaled_inputs=raw_data.iloc[1:,[2,4,5,6,7,9]]\n",
    "unscaled_targets=raw_data.iloc[1:,1]\n",
    "unscaled_test_data=pd.DataFrame(pd.read_csv('test.csv')).iloc[1:,[1,3,4,5,6,8]]\n",
    "#raw_data2=pd.DataFrame(unscaled_inputs)\n",
    "#raw_data3=pd.DataFrame(unscaled_targets)\n",
    "#raw_data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets=int(np.sum(unscaled_targets))\n",
    "\n",
    "num_zero_targets=0\n",
    "\n",
    "indices_to_remove=[]\n",
    "\n",
    "for i in range(unscaled_targets.shape[0]):\n",
    "    if unscaled_targets.iloc[i]== 0:\n",
    "         num_zero_targets += 1.\n",
    "    \n",
    "         if num_zero_targets > num_one_targets:\n",
    "                   indices_to_remove.append(i)\n",
    "    \n",
    "    \n",
    "unscaled_inputs_equal_priors=unscaled_inputs.drop(indices_to_remove,axis=0)\n",
    "unscaled_targets_equal_priors=unscaled_targets.drop(indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER M5\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\ACER M5\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "scaled_inputs=pd.DataFrame(preprocessing.scale(unscaled_inputs_equal_priors))\n",
    "scaled_test_data=pd.DataFrame(preprocessing.scale(unscaled_test_data))\n",
    "\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs.iloc[shuffled_indices]\n",
    "shuffled_targets = unscaled_targets_equal_priors.iloc[shuffled_indices]\n",
    "\n",
    "#shuffled_indices_for_test_data=[]\n",
    "\n",
    "#for i in range(400):\n",
    "     #if shuffled_indices[i] <= unscaled_test_data.shape[0]:\n",
    "            #shuffled_indices_for_targets.append(i)\n",
    "            \n",
    "#shuffled_test_data=unscaled_test_data.iloc[shuffled_indices_for_test_data]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215 547 0.3930530164533821\n",
      "50 136 0.36764705882352944\n"
     ]
    }
   ],
   "source": [
    "sample_count=shuffled_inputs.shape[0]\n",
    "\n",
    "train_sample_count=int(0.8 * sample_count)\n",
    "validation_sample_count=int(0.2*sample_count)\n",
    "\n",
    "train_inputs=shuffled_inputs.iloc[:train_sample_count]\n",
    "train_targets=shuffled_targets.iloc[:train_sample_count]\n",
    "\n",
    "validation_inputs=shuffled_inputs.iloc[train_sample_count:train_sample_count+validation_sample_count]\n",
    "validation_targets=shuffled_targets.iloc[train_sample_count:train_sample_count+validation_sample_count]\n",
    "\n",
    "test_sample_count=scaled_test_data.shape[0]\n",
    "test_inputs=scaled_test_data\n",
    "#test_targets=shuffled_targets.iloc[:shuffled_test_data.shape[0]]\n",
    "\n",
    "print(np.sum(train_targets), train_sample_count, np.sum(train_targets) / train_sample_count)\n",
    "print(np.sum(validation_targets), validation_sample_count, np.sum(validation_targets) / validation_sample_count)\n",
    "#print(np.sum(test_targets), test_sample_count, np.sum(test_targets) / test_sample_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Titanic_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Titanic_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Titanic_data_test', inputs=test_inputs)\n",
    "\n",
    "npz=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Titanic_Data_Reader():\n",
    "    \n",
    "\n",
    "    def __init__(self, dataset, batch_size = None):\n",
    "    \n",
    "        npz = np.load('Titanic_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "                 \n",
    "        if npz == 'Titanic_data_test':\n",
    "                \n",
    "                self.inputs= npz['inputs'].astype(np.float)\n",
    "                \n",
    "        \n",
    "        else:  \n",
    "                        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "        \n",
    "        if npz == 'Titanic_data_test':\n",
    "            batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "            inputs_batch = self.inputs[batch_slice]\n",
    "            self.curr_batch += 1\n",
    "            return inputs_batch\n",
    "                 \n",
    "        else:  \n",
    "                        \n",
    "            batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "            inputs_batch = self.inputs[batch_slice]\n",
    "            self.curr_batch += 1\n",
    "            targets_batch = self.targets[batch_slice]\n",
    "            self.curr_batch += 1\n",
    "            classes_num = 2\n",
    "            targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "            targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        \n",
    "            return inputs_batch, targets_one_hot\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "    def __iter__(self):\n",
    "            return self\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Training loss: 0.548. Validation loss: 0.925. Validation accuracy: 63.24%\n",
      "Epoch 2. Training loss: 0.535. Validation loss: 0.905. Validation accuracy: 63.24%\n",
      "Epoch 3. Training loss: 0.523. Validation loss: 0.886. Validation accuracy: 63.24%\n",
      "Epoch 4. Training loss: 0.512. Validation loss: 0.867. Validation accuracy: 63.24%\n",
      "Epoch 5. Training loss: 0.500. Validation loss: 0.848. Validation accuracy: 63.24%\n",
      "Epoch 6. Training loss: 0.489. Validation loss: 0.831. Validation accuracy: 63.24%\n",
      "Epoch 7. Training loss: 0.478. Validation loss: 0.814. Validation accuracy: 63.24%\n",
      "Epoch 8. Training loss: 0.468. Validation loss: 0.798. Validation accuracy: 63.24%\n",
      "Epoch 9. Training loss: 0.458. Validation loss: 0.782. Validation accuracy: 63.24%\n",
      "Epoch 10. Training loss: 0.448. Validation loss: 0.767. Validation accuracy: 63.24%\n",
      "Epoch 11. Training loss: 0.439. Validation loss: 0.753. Validation accuracy: 63.24%\n",
      "Epoch 12. Training loss: 0.431. Validation loss: 0.740. Validation accuracy: 63.24%\n",
      "Epoch 13. Training loss: 0.422. Validation loss: 0.728. Validation accuracy: 63.24%\n",
      "Epoch 14. Training loss: 0.414. Validation loss: 0.716. Validation accuracy: 63.24%\n",
      "Epoch 15. Training loss: 0.407. Validation loss: 0.705. Validation accuracy: 63.24%\n",
      "Epoch 16. Training loss: 0.400. Validation loss: 0.694. Validation accuracy: 63.24%\n",
      "Epoch 17. Training loss: 0.393. Validation loss: 0.685. Validation accuracy: 63.24%\n",
      "Epoch 18. Training loss: 0.387. Validation loss: 0.676. Validation accuracy: 63.24%\n",
      "Epoch 19. Training loss: 0.381. Validation loss: 0.667. Validation accuracy: 63.24%\n",
      "Epoch 20. Training loss: 0.376. Validation loss: 0.659. Validation accuracy: 63.24%\n",
      "Epoch 21. Training loss: 0.371. Validation loss: 0.652. Validation accuracy: 63.24%\n",
      "Epoch 22. Training loss: 0.366. Validation loss: 0.645. Validation accuracy: 63.24%\n",
      "Epoch 23. Training loss: 0.361. Validation loss: 0.639. Validation accuracy: 63.97%\n",
      "Epoch 24. Training loss: 0.357. Validation loss: 0.633. Validation accuracy: 63.97%\n",
      "Epoch 25. Training loss: 0.353. Validation loss: 0.628. Validation accuracy: 63.97%\n",
      "Epoch 26. Training loss: 0.349. Validation loss: 0.623. Validation accuracy: 63.97%\n",
      "Epoch 27. Training loss: 0.346. Validation loss: 0.618. Validation accuracy: 63.97%\n",
      "Epoch 28. Training loss: 0.342. Validation loss: 0.614. Validation accuracy: 64.71%\n",
      "Epoch 29. Training loss: 0.339. Validation loss: 0.610. Validation accuracy: 64.71%\n",
      "Epoch 30. Training loss: 0.336. Validation loss: 0.606. Validation accuracy: 64.71%\n",
      "Epoch 31. Training loss: 0.334. Validation loss: 0.603. Validation accuracy: 65.44%\n",
      "Epoch 32. Training loss: 0.331. Validation loss: 0.599. Validation accuracy: 65.44%\n",
      "Epoch 33. Training loss: 0.328. Validation loss: 0.596. Validation accuracy: 64.71%\n",
      "Epoch 34. Training loss: 0.326. Validation loss: 0.593. Validation accuracy: 64.71%\n",
      "Epoch 35. Training loss: 0.324. Validation loss: 0.590. Validation accuracy: 65.44%\n",
      "Epoch 36. Training loss: 0.322. Validation loss: 0.588. Validation accuracy: 65.44%\n",
      "Epoch 37. Training loss: 0.319. Validation loss: 0.585. Validation accuracy: 65.44%\n",
      "Epoch 38. Training loss: 0.317. Validation loss: 0.583. Validation accuracy: 66.91%\n",
      "Epoch 39. Training loss: 0.316. Validation loss: 0.581. Validation accuracy: 68.38%\n",
      "Epoch 40. Training loss: 0.314. Validation loss: 0.578. Validation accuracy: 69.85%\n",
      "Epoch 41. Training loss: 0.312. Validation loss: 0.576. Validation accuracy: 69.85%\n",
      "Epoch 42. Training loss: 0.310. Validation loss: 0.574. Validation accuracy: 70.59%\n",
      "Epoch 43. Training loss: 0.309. Validation loss: 0.572. Validation accuracy: 70.59%\n",
      "Epoch 44. Training loss: 0.307. Validation loss: 0.570. Validation accuracy: 71.32%\n",
      "Epoch 45. Training loss: 0.305. Validation loss: 0.569. Validation accuracy: 71.32%\n",
      "Epoch 46. Training loss: 0.304. Validation loss: 0.567. Validation accuracy: 70.59%\n",
      "Epoch 47. Training loss: 0.302. Validation loss: 0.565. Validation accuracy: 70.59%\n",
      "Epoch 48. Training loss: 0.301. Validation loss: 0.563. Validation accuracy: 69.85%\n",
      "Epoch 49. Training loss: 0.299. Validation loss: 0.562. Validation accuracy: 69.85%\n",
      "Epoch 50. Training loss: 0.298. Validation loss: 0.560. Validation accuracy: 70.59%\n",
      "Epoch 51. Training loss: 0.296. Validation loss: 0.559. Validation accuracy: 71.32%\n",
      "Epoch 52. Training loss: 0.295. Validation loss: 0.557. Validation accuracy: 71.32%\n",
      "Epoch 53. Training loss: 0.294. Validation loss: 0.556. Validation accuracy: 70.59%\n",
      "Epoch 54. Training loss: 0.292. Validation loss: 0.554. Validation accuracy: 70.59%\n",
      "Epoch 55. Training loss: 0.291. Validation loss: 0.553. Validation accuracy: 70.59%\n",
      "Epoch 56. Training loss: 0.290. Validation loss: 0.551. Validation accuracy: 72.06%\n",
      "Epoch 57. Training loss: 0.289. Validation loss: 0.550. Validation accuracy: 72.79%\n",
      "Epoch 58. Training loss: 0.287. Validation loss: 0.549. Validation accuracy: 72.06%\n",
      "Epoch 59. Training loss: 0.286. Validation loss: 0.547. Validation accuracy: 72.06%\n",
      "Epoch 60. Training loss: 0.285. Validation loss: 0.546. Validation accuracy: 72.79%\n",
      "Epoch 61. Training loss: 0.284. Validation loss: 0.545. Validation accuracy: 72.79%\n",
      "Epoch 62. Training loss: 0.283. Validation loss: 0.544. Validation accuracy: 72.79%\n",
      "Epoch 63. Training loss: 0.282. Validation loss: 0.543. Validation accuracy: 74.26%\n",
      "Epoch 64. Training loss: 0.281. Validation loss: 0.542. Validation accuracy: 73.53%\n",
      "Epoch 65. Training loss: 0.280. Validation loss: 0.541. Validation accuracy: 72.79%\n",
      "Epoch 66. Training loss: 0.278. Validation loss: 0.540. Validation accuracy: 73.53%\n",
      "Epoch 67. Training loss: 0.277. Validation loss: 0.539. Validation accuracy: 73.53%\n",
      "Epoch 68. Training loss: 0.276. Validation loss: 0.538. Validation accuracy: 73.53%\n",
      "Epoch 69. Training loss: 0.275. Validation loss: 0.537. Validation accuracy: 74.26%\n",
      "Epoch 70. Training loss: 0.274. Validation loss: 0.536. Validation accuracy: 74.26%\n",
      "Epoch 71. Training loss: 0.274. Validation loss: 0.535. Validation accuracy: 74.26%\n",
      "Epoch 72. Training loss: 0.273. Validation loss: 0.534. Validation accuracy: 74.26%\n",
      "Epoch 73. Training loss: 0.272. Validation loss: 0.533. Validation accuracy: 75.00%\n",
      "Epoch 74. Training loss: 0.271. Validation loss: 0.532. Validation accuracy: 75.00%\n",
      "Epoch 75. Training loss: 0.270. Validation loss: 0.532. Validation accuracy: 75.00%\n",
      "Epoch 76. Training loss: 0.269. Validation loss: 0.531. Validation accuracy: 75.00%\n",
      "Epoch 77. Training loss: 0.268. Validation loss: 0.530. Validation accuracy: 75.00%\n",
      "Epoch 78. Training loss: 0.267. Validation loss: 0.530. Validation accuracy: 74.26%\n",
      "Epoch 79. Training loss: 0.267. Validation loss: 0.529. Validation accuracy: 74.26%\n",
      "Epoch 80. Training loss: 0.266. Validation loss: 0.528. Validation accuracy: 75.00%\n",
      "Epoch 81. Training loss: 0.265. Validation loss: 0.528. Validation accuracy: 75.74%\n",
      "Epoch 82. Training loss: 0.264. Validation loss: 0.527. Validation accuracy: 75.74%\n",
      "Epoch 83. Training loss: 0.263. Validation loss: 0.527. Validation accuracy: 76.47%\n",
      "Epoch 84. Training loss: 0.263. Validation loss: 0.526. Validation accuracy: 76.47%\n",
      "Epoch 85. Training loss: 0.262. Validation loss: 0.525. Validation accuracy: 76.47%\n",
      "Epoch 86. Training loss: 0.261. Validation loss: 0.525. Validation accuracy: 76.47%\n",
      "Epoch 87. Training loss: 0.261. Validation loss: 0.524. Validation accuracy: 76.47%\n",
      "Epoch 88. Training loss: 0.260. Validation loss: 0.524. Validation accuracy: 76.47%\n",
      "Epoch 89. Training loss: 0.259. Validation loss: 0.524. Validation accuracy: 76.47%\n",
      "Epoch 90. Training loss: 0.259. Validation loss: 0.523. Validation accuracy: 76.47%\n",
      "Epoch 91. Training loss: 0.258. Validation loss: 0.523. Validation accuracy: 76.47%\n",
      "Epoch 92. Training loss: 0.257. Validation loss: 0.522. Validation accuracy: 76.47%\n",
      "Epoch 93. Training loss: 0.257. Validation loss: 0.522. Validation accuracy: 76.47%\n",
      "Epoch 94. Training loss: 0.256. Validation loss: 0.522. Validation accuracy: 76.47%\n",
      "Epoch 95. Training loss: 0.255. Validation loss: 0.521. Validation accuracy: 76.47%\n",
      "Epoch 96. Training loss: 0.255. Validation loss: 0.521. Validation accuracy: 77.21%\n",
      "Epoch 97. Training loss: 0.254. Validation loss: 0.521. Validation accuracy: 77.94%\n",
      "Epoch 98. Training loss: 0.254. Validation loss: 0.521. Validation accuracy: 77.94%\n",
      "Epoch 99. Training loss: 0.253. Validation loss: 0.520. Validation accuracy: 77.94%\n",
      "Epoch 100. Training loss: 0.253. Validation loss: 0.520. Validation accuracy: 77.94%\n",
      "Epoch 101. Training loss: 0.252. Validation loss: 0.520. Validation accuracy: 77.21%\n",
      "Epoch 102. Training loss: 0.252. Validation loss: 0.520. Validation accuracy: 77.21%\n",
      "Epoch 103. Training loss: 0.251. Validation loss: 0.519. Validation accuracy: 77.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104. Training loss: 0.251. Validation loss: 0.519. Validation accuracy: 77.21%\n",
      "Epoch 105. Training loss: 0.250. Validation loss: 0.519. Validation accuracy: 77.21%\n",
      "Epoch 106. Training loss: 0.250. Validation loss: 0.519. Validation accuracy: 77.94%\n",
      "Epoch 107. Training loss: 0.249. Validation loss: 0.519. Validation accuracy: 77.94%\n",
      "Epoch 108. Training loss: 0.249. Validation loss: 0.519. Validation accuracy: 77.94%\n",
      "Epoch 109. Training loss: 0.248. Validation loss: 0.519. Validation accuracy: 78.68%\n",
      "Epoch 110. Training loss: 0.248. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 111. Training loss: 0.248. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 112. Training loss: 0.247. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 113. Training loss: 0.247. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 114. Training loss: 0.246. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 115. Training loss: 0.246. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 116. Training loss: 0.246. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 117. Training loss: 0.245. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 118. Training loss: 0.245. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 119. Training loss: 0.244. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 120. Training loss: 0.244. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 121. Training loss: 0.244. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 122. Training loss: 0.243. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 123. Training loss: 0.243. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "Epoch 124. Training loss: 0.243. Validation loss: 0.518. Validation accuracy: 78.68%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "input_size = 6\n",
    "output_size = 2\n",
    "hidden_layer_size = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "outputs_2 = tf.nn.sigmoid(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(mean_loss)\n",
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 60\n",
    "max_epochs = 150\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    " \n",
    "train_data = Titanic_Data_Reader('train', batch_size)\n",
    "validation_data = Titanic_Data_Reader('validation')\n",
    "\n",
    " \n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "    \n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    curr_epoch_loss /= train_data.batch_count\n",
    "    \n",
    "    \n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in validation_data:\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    \n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    \n",
    "    prev_validation_loss = validation_loss\n",
    "    \n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'targets is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-20edf4d1e582>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTitanic_Data_Reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnew_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         test_accuracy = sess.run([ accuracy],\n\u001b[0;32m      5\u001b[0m             feed_dict={inputs: input_batch})\n",
      "\u001b[1;32m<ipython-input-7-55f8a4240b0b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnpz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inputs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'targets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not a file in the archive\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'targets is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "test_data=Titanic_Data_Reader('test')\n",
    "new_targets=[]\n",
    "for input_batch in test_data:\n",
    "        test_accuracy = sess.run([ accuracy],\n",
    "            feed_dict={inputs: input_batch})\n",
    "        \n",
    "test_accuracy_percent=test_accuracy[0] * 100\n",
    "print('Test Accuracy'.format(str(test_accuracy_percent)+'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
